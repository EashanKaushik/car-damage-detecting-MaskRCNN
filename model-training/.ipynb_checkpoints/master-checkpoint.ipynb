{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eashan\\Anaconda3\\envs\\mask_rcnn\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import basic libraries\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# import advance libraries\n",
    "from xml.etree import ElementTree\n",
    "import skimage.draw\n",
    "import cv2\n",
    "import imgaug\n",
    "\n",
    "# import mask rcnn libraries\n",
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.utils import extract_bboxes\n",
    "from mrcnn.utils import compute_ap\n",
    "from mrcnn.model import load_image_gt\n",
    "from mrcnn.model import mold_image\n",
    "from mrcnn import visualize\n",
    "\n",
    "# import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy libraries\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "\n",
    "# import keras libraries\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-1 Training (Single Class & Bounding Box Annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stage-1 training I used a simple dataset with images annotated using Bounding Boxes, and one class namely 'Damage'. In the following section you can find the code for training of the model. I have included comments to best desccribe the flow of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DamageDataset(Dataset):\n",
    "    \n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # we add a class that we need to classify in our case it is Damage\n",
    "        self.add_class(\"dataset\", 1, \"Damage\")\n",
    "        \n",
    "        # we concatenate the dataset_dir with /images and /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            # if is_train is True skip all images with id greater than and equal to 420\n",
    "            # roughly 80% of dataset for training\n",
    "            if is_train and int(image_id) >= 420 :\n",
    "                continue\n",
    "            \n",
    "            # if is_train is not True skip all images with id less than 420\n",
    "            if not is_train and int(image_id) < 420:\n",
    "                continue\n",
    "            \n",
    "            # declaring image path and annotations path\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training or testing\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "        \n",
    "        # <annotation>\n",
    "        # <size>\n",
    "\n",
    "        #       <width>640</width>\n",
    "\n",
    "        #       <height>360</height>\n",
    "\n",
    "        #       <depth>3</depth>\n",
    "\n",
    "        # </size>\n",
    "\n",
    "\n",
    "        # <object>\n",
    "\n",
    "        #          <name>damage</name>\n",
    "\n",
    "        #          <pose>Unspecified</pose>\n",
    "\n",
    "        #          <truncated>0</truncated>\n",
    "\n",
    "        #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "        #          <bndbox>\n",
    "\n",
    "        #                 <xmin>315</xmin>\n",
    "\n",
    "        #                 <ymin>160</ymin>\n",
    "\n",
    "        #                 <xmax>381</xmax>\n",
    "\n",
    "        #                 <ymax>199</ymax>\n",
    "\n",
    "        #          </bndbox>\n",
    "\n",
    "        # </object>\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in image\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        # we append the class_id 1 for Damage in our case to the variable\n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example:  if 2.jpg have three objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000001110 \n",
    "        # 000011100 011100000 000001110\n",
    "        # 000011100 011100000 000001110\n",
    "        # 000000000 011100000 000000000\n",
    "        #    1         1          1    <- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('Damage'))\n",
    "        \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class DamageConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"damage_cfg\"\n",
    "    \n",
    "    # damage class + background class\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 361\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# load the train dataset\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset('customImages/stage-1', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# load the test dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset('customImages/stage-1', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# prepare config by calling the user defined confifuration class\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-2 (Mulptiple Class and Bounding Box Annotaions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the stage I trained the model on multiple classes namely level-1 (scratch), level-2 (dent), level-3 (shatter), and level-4 (dislocation). The images are annotated using bounding boxes.\n",
    "\n",
    "I recommend going through Stag-1 Training code first, there is not much difference between code of stage 1 and stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DamageDataset(Dataset):\n",
    "\n",
    "    # load_dataset function is used to load the train and test dataset\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # we use add_class for each class in our dataset and assign numbers to them. 0 is background\n",
    "        # self.add_class('source', 'class id', 'class name')\n",
    "        self.add_class(\"dataset\", 1, \"Level-1\")\n",
    "        self.add_class(\"dataset\", 2, \"Level-2\")\n",
    "        self.add_class(\"dataset\", 3, \"Level-3\")\n",
    "        self.add_class(\"dataset\", 4, \"Level-4\")\n",
    "        \n",
    "        # we concatenate the dataset_dir with /images and /annots\n",
    "        images_dir = dataset_dir + '/images/'\n",
    "        annotations_dir = dataset_dir + '/annots/'\n",
    "        \n",
    "        # is_train will be true if we our training our model and false when we are testing the model\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_id = filename[:-4] # used to skip last 4 chars which is '.jpg' (class_id.jpg)\n",
    "            \n",
    "            # if is_train is True skip all images with id greater than and equal to 160\n",
    "             # roughly 80% of dataset for training\n",
    "            if is_train and int(image_id) >= 160 :\n",
    "                continue\n",
    "             \n",
    "            # if is_train is not True skip all images with id less than 420\n",
    "            if not is_train and int(image_id) < 160:\n",
    "                continue\n",
    "            \n",
    "            # img_path and ann_path variables are defined\n",
    "            img_path = images_dir + filename\n",
    "            ann_path = annotations_dir + image_id + '.xml'\n",
    "            \n",
    "            # using add_image function we pass image_id, image_path and ann_path so that the current\n",
    "            # image is added to the dataset for training\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "\n",
    "    \n",
    "    # function used to extract bouding boxes from annotated files\n",
    "    def extract_boxes(self, filename):\n",
    "\n",
    "        # you can see how the images are annotated we extracrt the width, height and bndbox values\n",
    "\n",
    "        # <size>\n",
    "\n",
    "        #       <width>640</width>\n",
    "\n",
    "        #       <height>360</height>\n",
    "\n",
    "        #       <depth>3</depth>\n",
    "\n",
    "        # </size>\n",
    "\n",
    "\n",
    "        # <object>\n",
    "\n",
    "        #          <name>damage</name>\n",
    "\n",
    "        #          <pose>Unspecified</pose>\n",
    "\n",
    "        #          <truncated>0</truncated>\n",
    "\n",
    "        #          <difficult>0</difficult>\n",
    "\n",
    "\n",
    "        #          <bndbox>\n",
    "\n",
    "        #                 <xmin>315</xmin>\n",
    "\n",
    "        #                 <ymin>160</ymin>\n",
    "\n",
    "        #                 <xmax>381</xmax>\n",
    "\n",
    "        #                 <ymax>199</ymax>\n",
    "\n",
    "        #          </bndbox>\n",
    "\n",
    "        # </object>\n",
    "\n",
    "        # </annotation>\n",
    "        \n",
    "        # used to parse the .xml files\n",
    "        tree = ElementTree.parse(filename)\n",
    "        \n",
    "        # to get the root of the xml file\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # we will append all x, y coordinated in boxes\n",
    "        # for all instances of an onject\n",
    "        boxes = list()\n",
    "        \n",
    "        # we find all attributes with name bndbox\n",
    "        # bndbox will exist for each ground truth in an image\n",
    "        for box in root.findall('.//object'):\n",
    "            name = box.find('name').text\n",
    "            xmin = int(box.find('./bndbox/xmin').text)\n",
    "            ymin = int(box.find('./bndbox/ymin').text)\n",
    "            xmax = int(box.find('./bndbox/xmax').text)\n",
    "            ymax = int(box.find('./bndbox/ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax, name]\n",
    "            boxes.append(coors)\n",
    "            \n",
    "            # I have included this line to skip any un-annotated images\n",
    "            if name=='Level-1' or name=='Level-2' or name=='Level-3' or name=='Level-4':\n",
    "                boxes.append(coors)\n",
    "\n",
    "        # extract width and height of the image\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        \n",
    "        # return boxes-> list, width-> int and height-> int \n",
    "        return boxes, width, height\n",
    "    \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances        \n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # we get the annotation path of image_id which is dataset_dir/annots/image_id.xml\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # we call the extract_boxes method(above) to get bndbox from .xml file\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "        \n",
    "        # we create len(boxes) number of masks of height 'h' and width 'w'\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "\n",
    "        class_ids = list()\n",
    "        \n",
    "        # we loop over all boxes and generate masks (bndbox mask) and class id for each instance\n",
    "        # masks will have rectange shape as we have used bndboxes for annotations\n",
    "        # for example: if 2.jpg have four objects we will have following masks and class_ids\n",
    "        # 000000000 000000000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000011100 022200000 000003330 111100000\n",
    "        # 000000000 022200000 000000000 000000000\n",
    "        #    1         2          3         1<- class_ids\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            \n",
    "            # box[4] will have the name of the class for a particular damage\n",
    "            if (box[4] == 'Level-1'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "                class_ids.append(self.class_names.index('Level-1'))\n",
    "            elif(box[4] == 'Level-2'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 2\n",
    "                class_ids.append(self.class_names.index('Level-2')) \n",
    "            elif(box[4] == 'Level-3'):\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 3\n",
    "                class_ids.append(self.class_names.index('Level-3'))\n",
    "            else:\n",
    "                masks[row_s:row_e, col_s:col_e, i] = 4\n",
    "                class_ids.append(self.class_names.index('Level-4'))\n",
    "                \n",
    "        # return masks and class_ids as array\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "# damage configuration class, you can change values of hyper parameters here\n",
    "class DamageConfig(Config):\n",
    "    # name of the configuration\n",
    "    NAME = \"damage_cfg\"\n",
    "    \n",
    "    #  background class + 4 classes\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    \n",
    "    # steps per epoch and minimum confidence\n",
    "    STEPS_PER_EPOCH = 160\n",
    "    \n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# prepare train set\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset('customImages/stage-2', is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "# validation/test\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset('customImages/stage-2', is_train=False)\n",
    "test_set.prepare()\n",
    "\n",
    "# load damage config\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, \n",
    "            test_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=5, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 ( Multiple Classes and images annotated with polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage I have used multiple classes namely scratch, dent, shatter and dislocation. The images are annotated using a polygon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DamageDataset(Dataset):\n",
    "\n",
    "    def load_dataset(self, dataset_dir, subset):\n",
    "        \n",
    "        # we use add_class for each class in our dataset and assign numbers to them. 0 is background\n",
    "        # self.add_class('source', 'class id', 'class name')\n",
    "        self.add_class(\"damage\", 1, \"Scratch\")\n",
    "        self.add_class(\"damage\", 2, \"Dent\")\n",
    "        self.add_class(\"damage\", 3, \"Shatter\")\n",
    "        self.add_class(\"damage\", 4, \"Dislocation\")\n",
    "        \n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        \n",
    "        # load annotations using json.load()\n",
    "        annotations1 = json.load(open(os.path.join(dataset_dir, \"via_project.json\")))\n",
    "        \n",
    "        # convert annotations1 into a list\n",
    "        annotations = list(annotations1.values())  \n",
    "        \n",
    "        # we only require the regions in the annotations\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            \n",
    "            # extracting shape attributes and region attributes\n",
    "            polygons = [r['shape_attributes'] for r in a['regions']] \n",
    "            objects = [s['region_attributes']['damage'] for s in a['regions']]\n",
    "            \n",
    "            # create a dictionary {name_of_class: class_id} remember background has id 0\n",
    "            name_dict = {\"Scratch\": 1, \"Dent\": 2, \"Shatter\": 3, \"Dislocation\": 4}\n",
    "            \n",
    "            # all the ids/classes in a image\n",
    "            num_ids = [name_dict[a] for a in objects]\n",
    "            \n",
    "            # you can print these ids\n",
    "            # print(\"numids\",num_ids)\n",
    "            \n",
    "            # read image and get height and width\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "            \n",
    "            # add image to the dataset\n",
    "            self.add_image(\n",
    "                \"damage\",\n",
    "                image_id=a['filename'],\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                num_ids=num_ids\n",
    "                )\n",
    "            \n",
    "    # this function calls on the extract_boxes method and is used to load a mask for each instance in an image\n",
    "    # returns a boolean mask with following dimensions width * height * instances\n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        # info points to the current image_id\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # for cases when source is not damage\n",
    "        if info[\"source\"] != \"damage\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        \n",
    "        # get the class ids in an image\n",
    "        num_ids = info['num_ids']\n",
    "        \n",
    "        \n",
    "        \n",
    "        # we create len(info[\"polygons\"])(total number of polygons) number of masks of height 'h' and width 'w'\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        \n",
    "        # we loop over all polygons and generate masks (polygon mask) and class id for each instance\n",
    "        # masks can have any shape as we have used polygon for annotations\n",
    "        # for example: if 2.jpg have four objects we will have following masks and class_ids\n",
    "        # 000001100 000111000 000001110\n",
    "        # 000111100 011100000 000001110\n",
    "        # 000011111 011111000 000001110\n",
    "        # 000000000 111100000 000000000\n",
    "        #    1         2          3    <- class_ids\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "\n",
    "            mask[rr, cc, i] = 1\n",
    "            \n",
    "        # return masks and class_ids as array\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\n",
    "        return mask, num_ids\n",
    "    \n",
    "    # this functions takes the image_id and returns the path of the image\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"damage\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n",
    "# define a configuration for the model\n",
    "class DamageConfig(Config):\n",
    "    # define the name of the configuration\n",
    "    NAME = \"damage\"\n",
    "    \n",
    "    # number of classes (background + damge classes)\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    \n",
    "    # number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 160\n",
    "    # learning rate and momentum\n",
    "    LEARNING_RATE=0.002\n",
    "    LEARNING_MOMENTUM = 0.8\n",
    "    \n",
    "    # regularization penalty\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    # image size is controlled by this parameter\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    \n",
    "    # validation steps\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # number of Region of Interest generated per image\n",
    "    Train_ROIs_Per_Image = 200\n",
    "    \n",
    "    # RPN Acnhor scales and ratios to find ROI\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 48, 64, 128)\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "# prepare train dataset.\n",
    "train_set = DamageDataset()\n",
    "# change the dataset \n",
    "train_set.load_dataset(\"customImages/stage-3\", \"train\")\n",
    "train_set.prepare()\n",
    "\n",
    "# prepare validation/test dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset(\"customImages/stage-3\", \"val\")\n",
    "test_set.prepare()\n",
    "\n",
    "# load damage config\n",
    "config = DamageConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
    "\n",
    "# load weights mscoco model weights\n",
    "weights_path = 'mask_rcnn_coco.h5'\n",
    "\n",
    "# load the model weights\n",
    "model.load_weights(weights_path, \n",
    "                   by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# start the training of model\n",
    "# you can change epochs and layers (head or all)\n",
    "model.train(train_set, \n",
    "            test_set, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=15, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation (Stage-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a prediction configuration \n",
    "class PredictionConfig(Config):\n",
    "    NAME = \"damage\"\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    DETECTION_MIN_CONFIDENCE = 0.85\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "\n",
    "# evaluate_model is used to calculate mean Average Precision of the model\n",
    "def evaluate_model(dataset, model, cfg):\n",
    "    APs = list()\n",
    "    for image_id in dataset.image_ids:\n",
    "\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
    "\n",
    "        scaled_image = mold_image(image, cfg)\n",
    "\n",
    "        sample = expand_dims(scaled_image, 0)\n",
    "\n",
    "        yhat = model.detect(sample, verbose=0)\n",
    "\n",
    "        r = yhat[0]\n",
    "\n",
    "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "\n",
    "        APs.append(AP)\n",
    "    maP = mean(APs)\n",
    "    return mAP\n",
    "\n",
    "# train dataset\n",
    "train_set = DamageDataset()\n",
    "train_set.load_dataset(\"customImages/stage-3\", \"train\")\n",
    "train_set.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "test_set = DamageDataset()\n",
    "test_set.load_dataset(\"customImages/stage-3\", \"val\")\n",
    "test_set.prepare()\n",
    "\n",
    "# create config\n",
    "cfg = PredictionConfig()\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
    "# load model weights\n",
    "model.load_weights('your_trained_model_weights.h5', by_name=True)\n",
    "\n",
    "# evaluate model on train dataset\n",
    "train_mAP = evaluate_model(train_set, model, cfg)\n",
    "print(\"Train mAP: %.3f\" % train_mAP[0])\n",
    "\n",
    "# evaluate model on test dataset\n",
    "test_mAP = evaluate_model(test_set, model, cfg)\n",
    "print(\"Test mAP: %.3f\" % test_mAP[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Image Prediction (Stage-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a configuration for the model\n",
    "class PredictionConfig(Config):\n",
    "    # define the name of the configuration\n",
    "    NAME = \"damage\"\n",
    "    # number of classes (background + kangaroo)\n",
    "    NUM_CLASSES = 1 + 4\n",
    "    # number of training steps per epoch\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction configuration\n",
    "cfg = PredictionConfig()\n",
    "\n",
    "# define the model\n",
    "model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
    "\n",
    "# load model weights\n",
    "model_path = 'your_trained_weights.h5'\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img(\"image_path.jpg\")\n",
    "image = img_to_array(image)\n",
    "\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "class_names = ['BG', 'Scratch', 'Dent', 'Shatter', 'Dislocation']\n",
    "\n",
    "r = results[0]\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names,  r['scores'])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
